{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Qustion Answering Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VQA Homepage http://visualqa.org/download.html\n",
    "\n",
    "Annotations taken from [Training annotations 2017 v2.0](http://visualqa.org/data/mscoco/vqa/v2_Annotations_Train_mscoco.zip)\n",
    "\n",
    "Questions taken from [Training questions 2017 v2.0](http://visualqa.org/data/mscoco/vqa/v2_Questions_Train_mscoco.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/vqa_examples.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import zipfile\n",
    "import random\n",
    "import numpy as np\n",
    "import h5py\n",
    "from collections import Counter, defaultdict\n",
    "from time import time\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import gzip\n",
    "import os\n",
    "\n",
    "GENSIM = True\n",
    "DATA_PATH = '..' + os.sep + 'NLP1-2017-VQA' + os.sep + 'data' + os.sep\n",
    "\n",
    "if GENSIM:\n",
    "    from gensim.models.keyedvectors import KeyedVectors\n",
    "    # Load Google's pre-trained Word2Vec model.\n",
    "    w2v_model = KeyedVectors.load_word2vec_format(DATA_PATH + 'GoogleNews-vectors-negative300.bin', binary=True, limit=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('./data/v2_Questions_Train_mscoco.zip', 'r') as file:\n",
    "    qdata = json.load(file.open(file.namelist()[0]))\n",
    "\n",
    "with zipfile.ZipFile('./data/v2_Annotations_Train_mscoco.zip', 'r') as file:\n",
    "    adata = json.load(file.open(file.namelist()[0])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Spelling correction (using Bing Speller) of question and answer strings\n",
    "* Question normalization (first char uppercase, last char ‘?’)\n",
    "* Answer normalization (all chars lowercase, no period except as decimal point, number words —> digits, strip articles (a, an the))\n",
    "* Adding apostrophe if a contraction is missing it (e.g., convert \"dont\" to \"don't\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9558cbf71d92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"# Datapoints: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'annotations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Datapoint keys: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'annotations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'adata' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"# Datapoints: \", len(adata['annotations']))\n",
    "print(\"Datapoint keys: \", adata['annotations'][0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some datapoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1:  {'question_type': 'what is this', 'multiple_choice_answer': 'net', 'answers': [{'answer': 'net', 'answer_confidence': 'maybe', 'answer_id': 1}, {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 2}, {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 3}, {'answer': 'netting', 'answer_confidence': 'yes', 'answer_id': 4}, {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 5}, {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 6}, {'answer': 'mesh', 'answer_confidence': 'maybe', 'answer_id': 7}, {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 8}, {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 9}, {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 10}], 'image_id': 458752, 'answer_type': 'other', 'question_id': 458752000}\n",
      "\n",
      "#2:  {'question_type': 'what', 'multiple_choice_answer': 'pitcher', 'answers': [{'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 1}, {'answer': 'catcher', 'answer_confidence': 'no', 'answer_id': 2}, {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 3}, {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 4}, {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 5}, {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 6}, {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 7}, {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 8}, {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 9}, {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 10}], 'image_id': 458752, 'answer_type': 'other', 'question_id': 458752001}\n",
      "\n",
      "#3:  {'answer_type': 'other', 'multiple_choice_answer': 'orange', 'answers': [{'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 1}, {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 2}, {'answer': 'orange', 'answer_confidence': 'maybe', 'answer_id': 3}, {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 4}, {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 5}, {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 6}, {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 7}, {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 8}, {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 9}, {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 10}], 'image_id': 458752, 'question_type': 'what color is the', 'question_id': 458752002}\n"
     ]
    }
   ],
   "source": [
    "print(\"#1: \", adata['annotations'][0])\n",
    "print(\"\\n#2: \", adata['annotations'][1])\n",
    "print(\"\\n#3: \", adata['annotations'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Datapoints:  443757\n",
      "\n",
      "Datapoint keys:  dict_keys(['image_id', 'question', 'question_id'])\n"
     ]
    }
   ],
   "source": [
    "print(\"# Datapoints: \", len(qdata['questions']))\n",
    "print(\"\\nDatapoint keys: \", qdata['questions'][0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1:  {'image_id': 458752, 'question': 'What is this photo taken looking through?', 'question_id': 458752000}\n",
      "\n",
      "#2:  {'image_id': 458752, 'question': 'What position is this man playing?', 'question_id': 458752001}\n",
      "\n",
      "#3:  {'image_id': 458752, 'question': 'What color is the players shirt?', 'question_id': 458752002}\n"
     ]
    }
   ],
   "source": [
    "print(\"#1: \", qdata['questions'][0])\n",
    "print(\"\\n#2: \", qdata['questions'][1])\n",
    "print(\"\\n#3: \", qdata['questions'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question_types = set()\n",
    "multiple_choice_answers = set()\n",
    "answer2count = defaultdict(int)\n",
    "answer_types = set()\n",
    "answertypes2count = defaultdict(int)\n",
    "top_answers_per_type = defaultdict(lambda: defaultdict(int))\n",
    "for ann in adata['annotations']:\n",
    "    question_types.add(ann['question_type'])\n",
    "    \n",
    "    multiple_choice_answers.add(ann['multiple_choice_answer'])\n",
    "    \n",
    "    answer2count[ann['multiple_choice_answer']] += 1\n",
    "    answer_types.add(ann['answer_type'])\n",
    "    \n",
    "    answertypes2count[ann['answer_type']] += 1\n",
    "    top_answers_per_type[ann['answer_type']][ann['multiple_choice_answer']] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Unique Question Types:  65\n",
      "{'is that a', 'is this', 'does this', 'is', 'what color is', 'is the person', 'what does the', 'what is the name', 'what sport is', 'what animal is', 'how many people are in', 'what number is', 'what is', 'is this an', 'what is in the', 'was', 'what color is the', 'what is the man', 'are these', 'what room is', 'is it', 'what', 'is the man', 'is there a', 'is he', 'what is on the', 'can you', 'what kind of', 'is this a', 'none of the above', 'why is the', 'what brand', 'where is the', 'are there any', 'which', 'what is the color of the', 'what color are the', 'what is the woman', 'who is', 'do', 'are the', 'what type of', 'what color', 'what are the', 'what time', 'is this person', 'are there', 'are', 'what is the person', 'what is this', 'how many people are', 'is the', 'where are the', 'could', 'has', 'how many', 'why', 'is there', 'what are', 'are they', 'do you', 'does the', 'is the woman', 'how', 'what is the'}\n"
     ]
    }
   ],
   "source": [
    "print(\"# Unique Question Types: \", len(question_types))\n",
    "print(question_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer Types:  {'other', 'number', 'yes/no'}\n",
      "Answer Type Counts:  [('other', 219269), ('yes/no', 166882), ('number', 57606)]\n",
      "\n",
      "Type 'other' Top 50 Answers [('white', 8915), ('blue', 5455), ('red', 5201), ('black', 5066), ('brown', 3814), ('green', 3750), ('yellow', 2792), ('gray', 2113), ('nothing', 1814), ('right', 1760), ('frisbee', 1641), ('baseball', 1597), ('left', 1563), ('none', 1562), ('tennis', 1502), ('wood', 1449), ('orange', 1425), ('bathroom', 1230), ('pizza', 1203), ('pink', 1201), ('kitchen', 1093), ('cat', 933), ('dog', 890), ('water', 888), ('man', 885), ('skateboarding', 884), ('grass', 879), ('skiing', 866), ('kite', 793), ('silver', 773), ('black and white', 766), ('surfing', 762), ('horse', 708), ('living room', 702), ('skateboard', 701), ('phone', 697), ('snow', 641), ('wii', 636), ('giraffe', 636), ('woman', 632), ('standing', 627), ('surfboard', 622), ('eating', 607), ('cake', 601), ('food', 599), ('apple', 586), ('sunny', 584), ('broccoli', 572), ('table', 564), ('hat', 557)]\n",
      "\n",
      "Type 'number' Top 50 Answers [('1', 12520), ('2', 12194), ('3', 6527), ('0', 4860), ('4', 4112), ('5', 2359), ('6', 1452), ('10', 972), ('7', 937), ('8', 907), ('12', 519), ('9', 514), ('20', 430), ('11', 360), ('15', 303), ('many', 258), ('25', 246), ('13', 242), ('30', 194), ('14', 175), ('50', 173), ('16', 147), ('100', 133), ('24', 127), ('18', 103), ('17', 82), ('40', 68), ('21', 64), ('200', 63), ('lot', 54), ('2 feet', 51), ('22', 51), ('19', 50), ('one way', 49), ('23', 49), ('27', 48), ('28', 45), ('38', 44), ('35', 42), ('10 feet', 38), ('55', 36), ('3 feet', 36), ('45', 33), ('26', 33), ('29', 31), ('60', 31), ('2010', 29), ('34', 27), ('32', 27), ('10:20', 27)]\n",
      "\n",
      "Type 'yes/no' Top 50 Answers [('yes', 84615), ('no', 82263), ('africa', 1), ('not', 1), ('cutting apples', 1), ('cups', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Answer Types: \", answer_types)\n",
    "print(\"Answer Type Counts: \", Counter(answertypes2count).most_common())\n",
    "for t in list(answer_types):\n",
    "    print(\"\\nType '%s' Top 50 Answers %s\" %(t, Counter(top_answers_per_type[t]).most_common(50)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Unique Answers:  22531\n",
      "\n",
      "Some Answers:  ['congress', 'colonials', 'dragonair', 'african american', 'cigarette', 'comic', 'chiquita and del monte', 'tilted', '3 days', 'cosmic ln', 'tennis clothes', 'emmanuel n photo', 'supply', 'mon-sat 8am-6pm', '007', 'love seat', 'medical', 'posing for photo', 'because they slaughter them for meat', 'syrup', 'changes in traffic', 'circus', 'green bay', 'airplanes', '488', 'taos', '2:14', 'coke and water', 'v', 'paddle', 'sheep and goat', '350', 'instruments', '05:04', 'building sandcastle', 'white, blue, and red', 'riding', 'on bed', 'housecat', 'roman', 'chicken, broccoli, pasta', 'taking stretch', 'spt', 'pillowcase', '617-497-4111', 'burlap', 'a place to stand', 'casino', '1890', 'crochet', 'no ball', 'tusk holes', 'eric berne', 'cake sale', 'chip wagon', 'stay back', '2 brunette', 'near city', 'at beach', 'under mom', 'independent', 'to play', 'boy on right', 'cleanliness', '1st base', 'wwwclaykessackcom', 'uphill', 'apple identification', 'side of road', 'table', 'cat in fridge', 'low fares', 'he likes it', 'finishing', '1971', 'thai cuisine', 'papasan', 'limes', 'becky mccray', '3002641', 'pearl', 'copyright', 'white,yellow,green, purple', 'it is other opening', 'woman on left', 'chest bump', 'gyro and fries', 'huge', '143', 'oil change', 'talking to friend', 'it was just cleaned', 'purchased it from store', 'theory and history of folklore', 'caught in food', 'on hot dog', 'directing plane', 'bungee cord', 'hamster', 'swimming trunks']\n",
      "\n",
      "Top 100 Common Answers:  [('yes', 84978), ('no', 82516), ('1', 12540), ('2', 12215), ('white', 8916), ('3', 6536), ('blue', 5455), ('red', 5201), ('black', 5066), ('0', 4977), ('4', 4118), ('brown', 3814), ('green', 3750), ('yellow', 2792), ('5', 2367), ('gray', 2113), ('nothing', 1814), ('right', 1766), ('frisbee', 1641), ('baseball', 1597), ('left', 1565), ('none', 1563), ('tennis', 1502), ('6', 1455), ('wood', 1449), ('orange', 1425), ('bathroom', 1230), ('pizza', 1203), ('pink', 1202), ('kitchen', 1093), ('10', 981), ('7', 938), ('cat', 933), ('8', 911), ('dog', 890), ('water', 888), ('man', 885), ('skateboarding', 884), ('grass', 879), ('skiing', 866), ('kite', 793), ('silver', 773), ('black and white', 766), ('surfing', 762), ('horse', 708), ('living room', 702), ('skateboard', 701), ('phone', 697), ('snow', 641), ('wii', 636), ('giraffe', 636), ('woman', 632), ('standing', 627), ('surfboard', 622), ('eating', 607), ('cake', 601), ('food', 599), ('apple', 586), ('sunny', 584), ('broccoli', 572), ('table', 564), ('hat', 557), ('stop', 556), ('purple', 546), ('laptop', 544), ('elephant', 539), ('12', 527), ('sheep', 520), ('9', 516), ('snowboarding', 510), ('train', 505), ('beach', 497), ('motorcycle', 490), ('soccer', 483), ('banana', 475), ('bus', 474), ('cow', 458), ('male', 455), ('trees', 454), ('walking', 443), ('umbrella', 442), ('winter', 437), ('20', 435), ('wine', 432), ('sitting', 420), ('flowers', 418), ('bear', 417), ('camera', 415), ('female', 413), ('tile', 407), ('metal', 400), ('clear', 400), ('tan', 396), ('many', 384), ('outside', 384), ('brick', 383), ('car', 377), ('plane', 365), ('11', 364), ('sandwich', 361)]\n"
     ]
    }
   ],
   "source": [
    "print(\"# Unique Answers: \", len(multiple_choice_answers))\n",
    "print(\"\\nSome Answers: \", list(np.random.choice(list(multiple_choice_answers), 100)))\n",
    "print(\"\\nTop 100 Common Answers: \", Counter(answer2count).most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subset will follow the same structure as the original VQA dataset. This is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Answer\n",
    "    * Question Type\n",
    "    * Majority Answer\n",
    "    * Answer Type\n",
    "    * Answer Candidates\n",
    "        * Given Answer\n",
    "        * Confidence\n",
    "        * Answerer ID\n",
    "        \n",
    "        \n",
    "* Question\n",
    "    * Question\n",
    "    * Image ID\n",
    "   \n",
    "   \n",
    "* Images\n",
    "    * ResNet Image Features (Size: 2048)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train your models on your machine with a CPU (or if you have a GPU), we need to reduce the size of the Dataset. We will reduce the original dataset in the following way:\n",
    "* 20k Q/A of answer type _yes/no_\n",
    "* 20k Q/A of answer type _number_\n",
    "* 20k Q/A of answer type _other_\n",
    "\n",
    "The total number of Q/A will then be 60000. We will divide into training, validation and test split. The ratio between the splits will be approximately: 80%, 15%, 5% respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Creation Looks good! Time Taken 2.33\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "idx = list(range(0,len(qdata['questions'])))\n",
    "random.seed(42)\n",
    "random.shuffle(idx)\n",
    "\n",
    "np.random.seed(42)\n",
    "splits = ['train', 'valid', 'test']\n",
    "\n",
    "n = 20000\n",
    "qdata_small = {'questions': list()}\n",
    "adata_small = {'annotations': list()}\n",
    "a_type_counts = {'yes/no': 0, 'number': 0, 'other': 0}\n",
    "\n",
    "while len(qdata_small['questions']) < 3*n:\n",
    "    i = idx.pop()\n",
    "    \n",
    "    at = adata['annotations'][i]['answer_type'] \n",
    "    \n",
    "    if a_type_counts[at] < n:\n",
    "        \n",
    "        if at == 'yes/no' and adata['annotations'][i]['multiple_choice_answer'] not in ['yes', 'no']:\n",
    "            continue\n",
    "            \n",
    "        adata_small['annotations'].append(adata['annotations'][i])\n",
    "        qdata_small['questions'].append(qdata['questions'][i])\n",
    "        \n",
    "        split = np.random.choice(splits, p=(.8, .15, .05))\n",
    "        adata_small['annotations'][-1]['split'] = split\n",
    "        qdata_small['questions'][-1]['split'] = split\n",
    "        \n",
    "        a_type_counts[at] += 1\n",
    "        \n",
    "# Tests\n",
    "assert len(qdata_small['questions']) == len(adata_small['annotations']) == 3*n, \"Inconsitent Lengths.\"\n",
    "a_type_counts = {'yes/no': 0, 'number': 0, 'other': 0}\n",
    "for ann in adata_small['annotations']:\n",
    "    a_type_counts[ann['answer_type']] += 1\n",
    "assert a_type_counts['yes/no'] == a_type_counts['number'] == a_type_counts['other'] == n, \"Inconsistent Answer Type Lengths.\"\n",
    "\n",
    "print(\"Data Creation Looks good! Time Taken %.2f\" %(time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some examples to verify this is the same data. Calculating the statistics again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annotations Small Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Datapoints:  60000\n",
      "\n",
      "Datapoint keys:  dict_keys(['question_type', 'multiple_choice_answer', 'answers', 'image_id', 'answer_type', 'question_id', 'split'])\n",
      "\n",
      "#1:  {'question_type': 'what', 'multiple_choice_answer': 'tea', 'answers': [{'answer': 'brunch', 'answer_confidence': 'maybe', 'answer_id': 1}, {'answer': 'tea', 'answer_confidence': 'yes', 'answer_id': 2}, {'answer': 'tea time', 'answer_confidence': 'yes', 'answer_id': 3}, {'answer': 'brunch', 'answer_confidence': 'yes', 'answer_id': 4}, {'answer': 'breakfast', 'answer_confidence': 'maybe', 'answer_id': 5}, {'answer': 'tea', 'answer_confidence': 'yes', 'answer_id': 6}, {'answer': 'teatime', 'answer_confidence': 'yes', 'answer_id': 7}, {'answer': 'lunch', 'answer_confidence': 'yes', 'answer_id': 8}, {'answer': 'reception', 'answer_confidence': 'maybe', 'answer_id': 9}, {'answer': 'breakfast', 'answer_confidence': 'yes', 'answer_id': 10}], 'image_id': 228478, 'answer_type': 'other', 'question_id': 228478002, 'split': 'train'}\n",
      "\n",
      "#2:  {'question_type': 'is there a', 'multiple_choice_answer': 'yes', 'answers': [{'answer': 'yes', 'answer_confidence': 'yes', 'answer_id': 1}, {'answer': 'yes', 'answer_confidence': 'yes', 'answer_id': 2}, {'answer': 'yes', 'answer_confidence': 'yes', 'answer_id': 3}, {'answer': 'yes', 'answer_confidence': 'yes', 'answer_id': 4}, {'answer': 'yes', 'answer_confidence': 'yes', 'answer_id': 5}, {'answer': 'yes', 'answer_confidence': 'yes', 'answer_id': 6}, {'answer': 'yes', 'answer_confidence': 'yes', 'answer_id': 7}, {'answer': 'yes', 'answer_confidence': 'yes', 'answer_id': 8}, {'answer': 'yes', 'answer_confidence': 'yes', 'answer_id': 9}, {'answer': 'yes', 'answer_confidence': 'yes', 'answer_id': 10}], 'image_id': 540769, 'answer_type': 'yes/no', 'question_id': 540769000, 'split': 'test'}\n",
      "\n",
      "#3:  {'question_type': 'what color is', 'multiple_choice_answer': 'blue', 'answers': [{'answer': 'blue', 'answer_confidence': 'yes', 'answer_id': 1}, {'answer': 'blue', 'answer_confidence': 'yes', 'answer_id': 2}, {'answer': 'blue', 'answer_confidence': 'yes', 'answer_id': 3}, {'answer': 'blue', 'answer_confidence': 'yes', 'answer_id': 4}, {'answer': 'blue', 'answer_confidence': 'yes', 'answer_id': 5}, {'answer': 'blue', 'answer_confidence': 'yes', 'answer_id': 6}, {'answer': 'blue', 'answer_confidence': 'yes', 'answer_id': 7}, {'answer': 'blue', 'answer_confidence': 'yes', 'answer_id': 8}, {'answer': 'blue', 'answer_confidence': 'yes', 'answer_id': 9}, {'answer': 'blue', 'answer_confidence': 'yes', 'answer_id': 10}], 'image_id': 111756, 'answer_type': 'other', 'question_id': 111756005, 'split': 'train'}\n"
     ]
    }
   ],
   "source": [
    "print(\"# Datapoints: \", len(adata_small['annotations']))\n",
    "print(\"\\nDatapoint keys: \", adata_small['annotations'][0].keys())\n",
    "print(\"\\n#1: \", adata_small['annotations'][0])\n",
    "print(\"\\n#2: \", adata_small['annotations'][1])\n",
    "print(\"\\n#3: \", adata_small['annotations'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions Small Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Datapoints:  60000\n",
      "\n",
      "Datapoint keys:  dict_keys(['image_id', 'question', 'question_id', 'split'])\n",
      "\n",
      "#1:  {'image_id': 228478, 'question': 'What English meal is this likely for?', 'question_id': 228478002, 'split': 'train'}\n",
      "\n",
      "#2:  {'image_id': 540769, 'question': 'Is there a bell on the train?', 'question_id': 540769000, 'split': 'test'}\n",
      "\n",
      "#3:  {'image_id': 111756, 'question': 'What color is his uniform?', 'question_id': 111756005, 'split': 'train'}\n"
     ]
    }
   ],
   "source": [
    "print(\"# Datapoints: \", len(qdata_small['questions']))\n",
    "print(\"\\nDatapoint keys: \", qdata_small['questions'][0].keys())\n",
    "print(\"\\n#1: \", qdata_small['questions'][0])\n",
    "print(\"\\n#2: \", qdata_small['questions'][1])\n",
    "print(\"\\n#3: \", qdata_small['questions'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Statistics Small Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question_types = set()\n",
    "multiple_choice_answers = set()\n",
    "answer2count = defaultdict(int)\n",
    "answer_types = set()\n",
    "answertypes2count = defaultdict(int)\n",
    "top_answers_per_type = defaultdict(lambda: defaultdict(int))\n",
    "for ann in adata_small['annotations']:\n",
    "    question_types.add(ann['question_type'])\n",
    "    \n",
    "    multiple_choice_answers.add(ann['multiple_choice_answer'])\n",
    "    \n",
    "    answer2count[ann['multiple_choice_answer']] += 1\n",
    "    answer_types.add(ann['answer_type'])\n",
    "    \n",
    "    answertypes2count[ann['answer_type']] += 1\n",
    "    top_answers_per_type[ann['answer_type']][ann['multiple_choice_answer']] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quesiton Types Small Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Unique Question Types:  65\n",
      "{'is that a', 'is this', 'does this', 'what color is', 'is', 'is the person', 'what is the name', 'what sport is', 'what does the', 'what animal is', 'how many people are in', 'what number is', 'what is', 'is this an', 'what is in the', 'what color is the', 'was', 'what is the man', 'are these', 'what room is', 'is it', 'is there a', 'what', 'is the man', 'is he', 'what is on the', 'can you', 'what kind of', 'is this a', 'none of the above', 'why is the', 'what brand', 'where is the', 'are there any', 'which', 'what is the color of the', 'what color are the', 'who is', 'what is the woman', 'do', 'what type of', 'are the', 'what are the', 'what color', 'is this person', 'what time', 'are there', 'are', 'what is the person', 'what is this', 'how many people are', 'is the', 'where are the', 'could', 'has', 'how many', 'why', 'is there', 'what are', 'are they', 'do you', 'does the', 'is the woman', 'how', 'what is the'}\n"
     ]
    }
   ],
   "source": [
    "print(\"# Unique Question Types: \", len(question_types))\n",
    "print(question_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer Types Small Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer Types:  {'other', 'number', 'yes/no'}\n",
      "Answer Type Counts:  [('other', 20000), ('yes/no', 20000), ('number', 20000)]\n",
      "\n",
      "Type 'other' Top 50 Answers [('white', 823), ('red', 494), ('black', 460), ('blue', 449), ('green', 355), ('brown', 331), ('yellow', 266), ('gray', 190), ('right', 154), ('frisbee', 152), ('nothing', 151), ('left', 144), ('baseball', 134), ('none', 132), ('orange', 130), ('wood', 127), ('tennis', 123), ('pink', 119), ('pizza', 118), ('kitchen', 113), ('bathroom', 106), ('cat', 90), ('water', 86), ('dog', 85), ('skiing', 84), ('grass', 84), ('surfing', 80), ('skateboarding', 78), ('horse', 75), ('black and white', 74), ('kite', 73), ('surfboard', 72), ('silver', 71), ('man', 69), ('living room', 66), ('woman', 65), ('giraffe', 64), ('table', 63), ('wii', 61), ('apple', 58), ('snow', 58), ('phone', 57), ('skateboard', 56), ('hat', 56), ('broccoli', 54), ('snowboarding', 53), ('eating', 53), ('cow', 52), ('standing', 51), ('sunny', 50)]\n",
      "\n",
      "Type 'number' Top 50 Answers [('1', 4298), ('2', 4281), ('3', 2270), ('0', 1677), ('4', 1382), ('5', 817), ('6', 510), ('8', 337), ('7', 330), ('10', 321), ('12', 190), ('9', 170), ('11', 135), ('20', 134), ('15', 97), ('25', 89), ('13', 87), ('many', 81), ('30', 77), ('14', 61), ('50', 56), ('16', 53), ('24', 52), ('100', 50), ('18', 46), ('17', 35), ('21', 24), ('27', 23), ('200', 23), ('19', 19), ('2 feet', 19), ('40', 19), ('10 feet', 19), ('lot', 17), ('3 feet', 16), ('35', 16), ('22', 16), ('one way', 15), ('5 years', 14), ('23', 14), ('28', 13), ('2012', 12), ('55', 12), ('old', 12), ('38', 12), ('2016', 11), ('12:00', 11), ('10:20', 11), ('29', 10), ('2010', 10)]\n",
      "\n",
      "Type 'yes/no' Top 50 Answers [('yes', 10178), ('no', 9822)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Answer Types: \", answer_types)\n",
    "print(\"Answer Type Counts: \", Counter(answertypes2count).most_common())\n",
    "for t in list(answer_types):\n",
    "    print(\"\\nType '%s' Top 50 Answers %s\" %(t, Counter(top_answers_per_type[t]).most_common(50)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answers Small Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Unique Answers:  5691\n",
      "\n",
      "Some Answers:  ['38', 'lift', '6 5 4 3', 'happy 50th birthday', 'cutting board', '8 ft', 'cook', 'fresh oil', 'bakery', 'stars and hearts', 'street cleaner', 'ahc 442', 'colorado', 'owner', 'surfing', 'fashion show', 'mile', 'champion', 'headband', 'portable', 'luggage room', 'green and white', '3:10', 'rackets', '10:00 am', 'ducati', 'mocking', 'cemetery', 'grapefruit', 'fire department', 'movement', '2 people', 'hippie drum circle', 'fresh fruit', '7502', 'kite', 'relaxed', 'monday', \"o'neill\", 'on counter', '100% fatto mano', '365', 'cigar', 'brother', 'bob', '2:28', 'shaggy', 'kitty litter', 'carrot cake', 'horseback riding', 'sandwich and chips', '11:58', 'tennis dress', 'back left', '2 towels', 'hungry', 'behind head', 'james bond', '055', 'crouching', 'one sweet ride', 'fist', 'rainbow', '95', '0870 400 4000', 'boardwalk', '258', '592', '1126', 'bucket in shower', 'overpass', 'old fashioned', 'forsythia', \"1940's\", 'fast', 'tim hortons', 'jollibee', 'top secret', 'midair', 'screen', 'doggie lookout station', 't-rex', 'bl7 234', 'gadzoom', 'calf', 'to protect their hands', 'recliner', 'washing machine', 'horizon', 'headband', 'because he bites', '2009', 'amtrak', 'middleton 112', '3 stories', 'calico', 'fdny', 'blender will turn on', 'heinz', '1203']\n",
      "\n",
      "Top 100 Common Answers:  [('yes', 10207), ('no', 9845), ('1', 4300), ('2', 4281), ('3', 2270), ('0', 1690), ('4', 1382), ('white', 823), ('5', 818), ('6', 510), ('red', 494), ('black', 460), ('blue', 449), ('green', 355), ('8', 337), ('brown', 331), ('7', 330), ('10', 323), ('yellow', 266), ('12', 192), ('gray', 190), ('9', 170), ('right', 156), ('frisbee', 152), ('nothing', 151), ('left', 144), ('11', 135), ('20', 135), ('baseball', 134), ('none', 132), ('orange', 130), ('wood', 127), ('tennis', 123), ('pink', 119), ('pizza', 118), ('kitchen', 113), ('bathroom', 106), ('15', 97), ('many', 96), ('cat', 90), ('25', 90), ('13', 87), ('water', 86), ('dog', 85), ('skiing', 84), ('grass', 84), ('surfing', 80), ('skateboarding', 78), ('30', 77), ('horse', 75), ('black and white', 74), ('kite', 73), ('surfboard', 72), ('silver', 71), ('man', 69), ('living room', 66), ('woman', 65), ('giraffe', 64), ('table', 63), ('wii', 61), ('14', 61), ('apple', 58), ('snow', 58), ('phone', 57), ('skateboard', 56), ('hat', 56), ('50', 56), ('broccoli', 54), ('16', 53), ('snowboarding', 53), ('eating', 53), ('24', 52), ('cow', 52), ('standing', 51), ('sunny', 50), ('beach', 50), ('elephant', 50), ('laptop', 50), ('purple', 50), ('100', 50), ('food', 49), ('18', 47), ('soccer', 47), ('cake', 47), ('motorcycle', 45), ('winter', 45), ('flowers', 45), ('walking', 45), ('bus', 45), ('stop', 45), ('tile', 45), ('male', 43), ('train', 43), ('trees', 43), ('tan', 42), ('sheep', 42), ('camera', 41), ('bear', 40), ('metal', 38), ('kites', 38)]\n"
     ]
    }
   ],
   "source": [
    "print(\"# Unique Answers: \", len(multiple_choice_answers))\n",
    "print(\"\\nSome Answers: \", list(np.random.choice(list(multiple_choice_answers), 100)))\n",
    "print(\"\\nTop 100 Common Answers: \", Counter(answer2count).most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Size: 48061\n",
      "\n",
      "Validation Set Size: 8977\n",
      "\n",
      "Test Set Size: 2962\n"
     ]
    }
   ],
   "source": [
    "qdata_small_splits = {\\\n",
    "                      'train': {'questions': list()}, \n",
    "                      'valid': {'questions': list()}, \n",
    "                      'test': {'questions': list()}\n",
    "                     }\n",
    "\n",
    "adata_small_splits = {\\\n",
    "                      'train': {'annotations': list()}, \n",
    "                      'valid': {'annotations': list()}, \n",
    "                      'test': {'annotations': list()}\n",
    "                     }\n",
    "\n",
    "for i in range(len(qdata_small['questions'])):\n",
    "    \n",
    "    split = qdata_small['questions'][i]['split']\n",
    "    assert split == adata_small['annotations'][i]['split'], \"Inconsistent Splits.\"\n",
    "    assert adata_small['annotations'][i]['question_id'] == qdata_small['questions'][i]['question_id'], \"Inconsistent IDs.\"\n",
    "    \n",
    "    qdata_small_splits[split]['questions'].append(qdata_small['questions'][i])\n",
    "    adata_small_splits[split]['annotations'].append(adata_small['annotations'][i])\n",
    "    \n",
    "        \n",
    "print(\"Training Set Size: %i\" %(len(qdata_small_splits['train']['questions'])))\n",
    "print(\"\\nValidation Set Size: %i\" %(len(qdata_small_splits['valid']['questions'])))\n",
    "print(\"\\nTest Set Size: %i\" %(len(qdata_small_splits['test']['questions'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gzip' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a2e6742cfa08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'valid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/vqa_annotatons_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.gzip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madata_small_splits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gzip' is not defined"
     ]
    }
   ],
   "source": [
    "for split in ['train', 'valid', 'test']:\n",
    "    \n",
    "    with gzip.GzipFile('data/vqa_annotatons_' + split + '.gzip', 'w') as file:\n",
    "        file.write(json.dumps(adata_small_splits[split]).encode('utf-8'))\n",
    "        \n",
    "    with gzip.GzipFile('data/vqa_questions_' + split + '.gzip', 'w') as file:\n",
    "        file.write(json.dumps(qdata_small_splits[split]).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of all image ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-2-ac0637ffca19>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-ac0637ffca19>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    with gzip.open('file.txt.gz', 'rb') as f:\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "image_ids = set()\n",
    "for q in qdata_small['questions']:\n",
    "    image_ids.add(q['image_id'])\n",
    "\n",
    "image_ids_json = {'image_ids': list(image_ids)}\n",
    "with open('data/image_ids_vqa.json', 'w') as file:\n",
    "with gzip.open('file.txt.gz', 'rb') as f:\n",
    "    file_content = f.read()\n",
    "    json.dump(image_ids_json, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data\n",
    "with gzip.open(DATA_PATH + 'vqa_questions_train.gzip', 'rb') as file:\n",
    "    file_content = file.read().decode('utf-8')\n",
    "    qdata_train = json.loads(file_content)\n",
    "\n",
    "with gzip.GzipFile(DATA_PATH + 'vqa_annotatons_train.gzip', 'r') as file:\n",
    "    adata_train = json.loads(file.read().decode('utf-8')) \n",
    "    \n",
    "with gzip.open(DATA_PATH + 'vqa_questions_test.gzip', 'rb') as file:\n",
    "    file_content = file.read().decode('utf-8')\n",
    "    qdata_test = json.loads(file_content)\n",
    "    \n",
    "with gzip.GzipFile(DATA_PATH + 'vqa_annotatons_test.gzip', 'r') as file:\n",
    "    adata_test = json.loads(file.read().decode('utf-8')) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_id': 228478, 'question': 'What English meal is this likely for?', 'question_id': 228478002, 'split': 'train'}\n",
      "#1:  what\n"
     ]
    }
   ],
   "source": [
    "print(qdata_train['questions'][0])\n",
    "print(\"#1: \", adata_train['annotations'][0]['question_type'])\n",
    "\n",
    "question_types = set()\n",
    "multiple_choice_answers = set()\n",
    "answer2count = defaultdict(int)\n",
    "answer_types = set()\n",
    "answertypes2count = defaultdict(int)\n",
    "top_answers_per_type = defaultdict(lambda: defaultdict(int))\n",
    "for ann in adata_train['annotations']:\n",
    "    question_types.add(ann['question_type'])\n",
    "    \n",
    "    multiple_choice_answers.add(ann['multiple_choice_answer'])\n",
    "    \n",
    "    answer2count[ann['multiple_choice_answer']] += 1\n",
    "    answer_types.add(ann['answer_type'])\n",
    "    \n",
    "    answertypes2count[ann['answer_type']] += 1\n",
    "    top_answers_per_type[ann['answer_type']][ann['multiple_choice_answer']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "# Functions to read in the corpus\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "question_types_dict = defaultdict(lambda: len(question_types_dict))\n",
    "UNK = w2i[\"<unk>\"]\n",
    "\n",
    "\n",
    "def question_type_to_idx(question_types):\n",
    "    for question_type in (question_types):\n",
    "        question_types_dict[question_type]\n",
    "    return question_types_dict\n",
    "\n",
    "\n",
    "def read_data(train_data, train_answer_data):\n",
    "    for idx in range(len(train_data)):\n",
    "        question = train_data[idx]['question']\n",
    "        question = question.lower().split(\"?\",1)[0]\n",
    "        question_type = train_answer_data[idx]['question_type']\n",
    "        yield ([w2i[x] for x in question.split(\" \")], question_types_dict[question_type])\n",
    "        \n",
    "# transering the tag(question type) to index\n",
    "question_types_dict = question_type_to_idx(question_types)\n",
    "\n",
    "#get the train questions, and then transfer to index,also add the tag index for every quesiton into the list\n",
    "train = list(read_data(qdata_train['questions'], adata_train['annotations']))\n",
    "w2i = defaultdict(lambda: UNK, w2i)\n",
    "#get the test questions, and then transfer to index,also add the tag index for every quesiton into the list\n",
    "dev = list(read_data(qdata_test['questions'], adata_test['annotations']))\n",
    "\n",
    "nwords = len(w2i)\n",
    "ntags = len(question_types_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not GENSIM:\n",
    "    # The parameters for our BoW-model\n",
    "    dtype = torch.FloatTensor  # enable CUDA here if you like\n",
    "    w = Variable(torch.randn(nwords, ntags).type(dtype), requires_grad=True)\n",
    "    b = Variable(torch.randn(ntags).type(dtype), requires_grad=True)\n",
    "\n",
    "\n",
    "    # A function to calculate scores for one sentence\n",
    "    def calc_scores(words):\n",
    "        lookup_tensor = Variable(torch.LongTensor(words))\n",
    "        embed = w[lookup_tensor]\n",
    "        score = torch.sum(embed, 0) + b\n",
    "        return score.view((1, -1))\n",
    "\n",
    "    for ITER in range(10):\n",
    "\n",
    "        # train\n",
    "        random.shuffle(train)\n",
    "        train_loss = 0.0\n",
    "        start = time.time()\n",
    "        for words, tag in train:\n",
    "\n",
    "            # forward pass\n",
    "            scores = calc_scores(words)\n",
    "            target = Variable(torch.LongTensor([tag]))        \n",
    "            loss = nn.CrossEntropyLoss()\n",
    "            output = loss(scores, target)\n",
    "            train_loss += output.data[0]        \n",
    "\n",
    "            # backward pass (compute gradients)\n",
    "            output.backward()\n",
    "\n",
    "            # update weights with SGD\n",
    "            lr = 0.01\n",
    "            w.data -= lr * w.grad.data\n",
    "            b.data -= lr * b.grad.data\n",
    "\n",
    "            # clear gradients for next step\n",
    "            w.grad.data.zero_()\n",
    "            b.grad.data.zero_()\n",
    "\n",
    "        print(\"iter %r: train loss/sent=%.4f, time=%.2fs\" % \n",
    "              (ITER, train_loss/len(train), time.time()-start))\n",
    "\n",
    "        # evaluate\n",
    "        correct = 0.0\n",
    "        for words, tag in dev:\n",
    "            scores = calc_scores(words)\n",
    "            #print(scores)\n",
    "            predict = scores.data.numpy().argmax(axis=1)\n",
    "            if predict == tag:\n",
    "                correct += 1\n",
    "\n",
    "        print(\"iter %r: test acc=%.4f\" % \n",
    "              (ITER, correct/len(dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x7fa4315386a8>, {'is it': 0, 'is the man': 1, 'is he': 2, 'what is': 3, 'is that a': 4, 'what animal is': 5, 'is the woman': 6, 'what brand': 7, 'is this a': 8, 'is there': 9, 'are the': 10, 'is this person': 11, 'what': 12, 'what is the woman': 13, 'what color are the': 14, 'what kind of': 15, 'do you': 16, 'what is on the': 17, 'do': 18, 'is the person': 19, 'what color is': 20, 'is there a': 21, 'was': 22, 'what is the name': 23, 'why': 24, 'who is': 25, 'is this an': 26, 'does this': 27, 'has': 28, 'how': 29, 'what are': 30, 'what is the color of the': 31, 'what is the person': 32, 'is the': 33, 'can you': 34, 'what is this': 35, 'is': 36, 'are they': 37, 'none of the above': 38, 'is this': 39, 'are there any': 40, 'what sport is': 41, 'what time': 42, 'are there': 43, 'what is the man': 44, 'could': 45, 'what are the': 46, 'what does the': 47, 'how many': 48, 'are these': 49, 'what is in the': 50, 'why is the': 51, 'what color': 52, 'how many people are': 53, 'what number is': 54, 'where are the': 55, 'what room is': 56, 'which': 57, 'what is the': 58, 'what type of': 59, 'what color is the': 60, 'where is the': 61, 'does the': 62, 'how many people are in': 63, 'are': 64})\n"
     ]
    }
   ],
   "source": [
    "print(question_types_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not GENSIM:\n",
    "    # evaluate\n",
    "    i2w_dict = {}\n",
    "    def i2w():\n",
    "        for k, v in w2i.items():\n",
    "            i2w_dict[v] = k\n",
    "\n",
    "    def sent2i(sent):\n",
    "        sent = sent.lower().split(\"?\",1)[0]\n",
    "        yield ([w2i[x] for x in sent.split(\" \")])\n",
    "\n",
    "    sent = 'how are you ?'\n",
    "    test_sent = list(sent2i(sent))[0]\n",
    "    print(test_sent)\n",
    "    scores = calc_scores(test_sent)\n",
    "    #print(scores)\n",
    "    print(type(scores))\n",
    "    predict = scores.data.numpy().argmax(axis=1)\n",
    "    scores_numpy = scores.data.numpy()\n",
    "    print(scores_numpy.shape)\n",
    "    print(predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read from image\n",
    "path_to_h5_file = DATA_PATH + 'VQA_image_features.h5'\n",
    "path_to_json_file = DATA_PATH + 'VQA_img_features2id.json'\n",
    "\n",
    "\n",
    "img_features = np.asarray(h5py.File(path_to_h5_file, 'r')['img_features'])\n",
    "with open(path_to_json_file, 'r') as f:\n",
    "     visual_feat_mapping = json.load(f)['VQA_imgid2id']\n",
    "\n",
    "\n",
    "h5_id = visual_feat_mapping[str(228478)]\n",
    "img_feat = img_features[h5_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048,)\n",
      "(39423, 2048)\n"
     ]
    }
   ],
   "source": [
    "print(img_feat.shape)\n",
    "print(img_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number\n",
      "other\n",
      "123\n",
      "yes/no\n",
      "4949\n",
      "4878\n"
     ]
    }
   ],
   "source": [
    "#if not GENSIM:\n",
    "#softmax to cover the answer\n",
    "ans_soft_max = []\n",
    "for t in list(answer_types):\n",
    "    print(t)\n",
    "    top_answers = Counter(top_answers_per_type[t])\n",
    "    #trying to covering at most 80% possible answers\n",
    "    if (t == 'other'):\n",
    "        print('123')\n",
    "        covering_answer_index = 800#round((len(top_answers)*4/5))\n",
    "    else :\n",
    "        covering_answer_index = 100\n",
    "    covering_answer_index = round((len(top_answers)))\n",
    "    for x in top_answers.most_common(covering_answer_index):\n",
    "        ans_soft_max.append(x[0])\n",
    "    #print(ans_soft_max)\n",
    "    #print('--------------------------')\n",
    "\n",
    "\n",
    "print(len(ans_soft_max))\n",
    "print(len(Counter((ans_soft_max))))\n",
    "#print(multiple_choice_answers)\n",
    "#print(ans_soft_max)\n",
    "ans_soft_max = list(multiple_choice_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4878\n",
      "input_size 48061\n",
      "output len:  4878\n",
      "input len  2113\n",
      "4878\n",
      "123\n"
     ]
    }
   ],
   "source": [
    "#if not GENSIM:\n",
    "def answer_to_idx(ans_dict):\n",
    "    ans_to_idx = defaultdict(lambda: len(ans_to_idx))\n",
    "    for ans in (ans_dict):\n",
    "        ans_to_idx[ans]\n",
    "    return ans_to_idx\n",
    "\n",
    "#ans_to_idx_dict is a dictionary convering all 80% possible answers, which are representing in index\n",
    "ans_to_idx_dict = dict(answer_to_idx(ans_soft_max))\n",
    "#print('example: answer: tea to index', ans_to_idx_dict['teaaaaaaaaaaaaa!!!!!!!!!!!!'])\n",
    "\n",
    "#print('1' in ans_to_idx_dict)\n",
    "#network parameter\n",
    "print(len(Counter(ans_soft_max)))\n",
    "n_output = len(ans_to_idx_dict)\n",
    "n_input =  (img_features.shape[1]) + 65\n",
    "n_hidden_size = 10\n",
    "learning_rate = 0.0005\n",
    "input_size = len(adata_train['annotations'])\n",
    "\n",
    "print('input_size', input_size)\n",
    "print('output len: ', n_output)\n",
    "print('input len ', n_input)\n",
    "print(len(Counter(ans_soft_max)))\n",
    "if 'computer repair' in ans_to_idx_dict:\n",
    "    print('123')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#build network\n",
    "'''\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        if hidden_size == 0:\n",
    "            self.fc1 = nn.Linear(input_size, num_classes)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "            self.hidden_size = hidden_size\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "            #self.sigmoid = nn.Sigmoid()\n",
    "            self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "            #self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        hidden_size = 10\n",
    "        if hidden_size == 0:\n",
    "            out = self.fc1(x)\n",
    "            #out = self.sigmoid(out)\n",
    "        else:\n",
    "            out = self.fc1(x)\n",
    "            out = nn.functional.relu(out)\n",
    "            #out = self.sigmoid(out)\n",
    "            out = self.fc2(out)\n",
    "            #out = self.sigmoid(out)\n",
    "        return out\n",
    "'''\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer\n",
    "        self.out = torch.nn.Linear(n_hidden, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.hidden(x))      # activation function for hidden layer\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0  loss:  61980.55558633804\n",
      "err:  8264\n",
      "    1  loss:  52967.938830673695\n",
      "    2  loss:  50231.88807705045\n",
      "    3  loss:  48238.78715848923\n",
      "    4  loss:  46851.998824208975\n",
      "    5  loss:  45905.248926728964\n",
      "    6  loss:  45167.308651804924\n",
      "    7  loss:  44517.14457079768\n",
      "    8  loss:  43914.19646058977\n",
      "    9  loss:  43366.95204202831\n",
      "   10  loss:  42892.45006664097\n",
      "   11  loss:  42474.73805345595\n",
      "   12  loss:  42099.37783502042\n",
      "   13  loss:  41761.832625977695\n",
      "   14  loss:  41440.329620450735\n",
      "   15  loss:  41138.40046438575\n",
      "   16  loss:  40851.12202848494\n",
      "   17  loss:  40578.580074481666\n",
      "   18  loss:  40316.34079051018\n",
      "   19  loss:  40062.69250001013\n",
      "   20  loss:  39814.19047113508\n",
      "   21  loss:  39578.09054578468\n"
     ]
    }
   ],
   "source": [
    "model = Net(2348, n_hidden_size, n_output)\n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "#loss_fn = torch.nn.MultiLabelSoftMarginLoss()\n",
    "\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "temp_x = Variable()\n",
    "temp_y = Variable()\n",
    "\n",
    "for ITER in range(500):\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    #loss = []\n",
    "    #print('start!')\n",
    "    count_err = 0\n",
    "    for idx, adata in enumerate(adata_train['annotations'][0:100]):\n",
    "        #preparing input\n",
    "        #if idx % 100 == 0:\n",
    "        #    print(idx)\n",
    "        #get work vecotr\n",
    "        question = qdata_train['questions'][idx]['question']\n",
    "        #print(question)\n",
    "        \n",
    "        #question_word_vector = calc_scores(list(sent2i(question))[0]).data.numpy().reshape(65,)\n",
    "        \n",
    "        question_word_vector = None\n",
    "        question_split = question.split(' ')\n",
    "        for question_word in question_split:            \n",
    "            if question_word not in w2v_model:                                \n",
    "                question_word = question_word[:-1]\n",
    "                if question_word not in w2v_model:\n",
    "                    #print(question_word)\n",
    "                    continue\n",
    "            if question_word_vector is None:\n",
    "                question_word_vector = np.array(w2v_model[question_word])\n",
    "            else:\n",
    "                question_word_vector += np.array(w2v_model[question_word])\n",
    "            #print(question_word_vector.shape)\n",
    "        \n",
    "        zero_question_word_vector = np.zeros(300,)\n",
    "        #question_word_vector *= 10\n",
    "        #question_word_vector = zero_question_word_vector\n",
    "        #input('---------------')\n",
    "        #get image vector\n",
    "        h5_id = visual_feat_mapping[str(adata['image_id'])]\n",
    "        \n",
    "        zero_question_img_vector = np.zeros(2048,)\n",
    "        \n",
    "        img_feat = img_features[h5_id]\n",
    "        #img_feat = zero_question_img_vector\n",
    "        \n",
    "        #concatenate word vecotr and image vector\n",
    "        \n",
    "        img_word_vector = np.concatenate((question_word_vector, img_feat), axis=0)\n",
    "        #print(img_word_vector.shape)\n",
    "        if not(adata['multiple_choice_answer'] in ans_to_idx_dict):\n",
    "            continue\n",
    "        #output_vector = np.zeros((1,1))\n",
    "        #output_vector[ans_to_idx_dict[adata['multiple_choice_answer']]] = 1\n",
    "        output_vector = np.array([ans_to_idx_dict[adata['multiple_choice_answer']]])#.reshape(1,1)\n",
    "        x = Variable(torch.from_numpy(img_word_vector).type(torch.FloatTensor))#.cuda()\n",
    "        y = Variable(torch.from_numpy(output_vector))#.cuda()\n",
    "        \n",
    "        \n",
    "        y_pred = model(x).view(1,n_output)\n",
    "        if (ITER  % 100 == 0) or (ITER == 999):\n",
    "            pass\n",
    "            #print(question,' image id:', (adata['image_id']))\n",
    "            #print(ans_soft_max[y_pred.data.numpy().argmax()], adata['multiple_choice_answer'])\n",
    "            \n",
    "        if ans_soft_max[y_pred.data.numpy().argmax()] != adata['multiple_choice_answer']:\n",
    "            count_err += 1\n",
    "        #if idx % 1000 == 0:\n",
    "        #    print('idx ' + str(idx))\n",
    "        loss = loss_fn(y_pred, y)\n",
    "            \n",
    "        train_loss += loss.data[0]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()   \n",
    "        \n",
    "        #optimizer.zero_grad()\n",
    "        #loss.backward()\n",
    "        #optimizer.step()       \n",
    "        #loss.append()\n",
    "    \n",
    "    print('{:>5}'.format(ITER),' loss: ', train_loss)\n",
    "    if (ITER  % 100 == 0):\n",
    "        print('err: ', count_err)\n",
    "        pass\n",
    "        #print('{:>5}'.format(ITER),' loss: ', train_loss)\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where is the guy ?\n",
      "\n",
      "tea\n",
      "Variable containing:\n",
      " 22.1586\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "tea\n",
      "Variable containing:\n",
      " 21.3026\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "wii remote\n",
      "Variable containing:\n",
      " 1.3998\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://images.cocodataset.org/train2014/COCO_train2014_000000228478.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What English meal is this likely for?\n",
      "tea\n",
      "tea\n",
      "Variable containing:\n",
      " 23.3849\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "tea\n",
      "Variable containing:\n",
      " 21.3026\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "photo shoot\n",
      "Variable containing:\n",
      " 2.1975\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://images.cocodataset.org/train2014/COCO_train2014_000000228478.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where is the guy ?\n",
      "\n",
      "blue\n",
      "Variable containing:\n",
      " 28.8643\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "blue\n",
      "Variable containing:\n",
      " 28.6085\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "wii remote\n",
      "Variable containing:\n",
      " 1.3998\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://images.cocodataset.org/train2014/COCO_train2014_000000111756.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What color is his uniform?\n",
      "blue\n",
      "blue\n",
      "Variable containing:\n",
      " 29.1507\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "blue\n",
      "Variable containing:\n",
      " 28.6085\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "wii remote\n",
      "Variable containing:\n",
      " 2.2816\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://images.cocodataset.org/train2014/COCO_train2014_000000111756.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where is the guy ?\n",
      "\n",
      "right\n",
      "Variable containing:\n",
      " 21.6719\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "right\n",
      "Variable containing:\n",
      " 21.5865\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "wii remote\n",
      "Variable containing:\n",
      " 1.3998\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://images.cocodataset.org/train2014/COCO_train2014_000000376241.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which girl is wearing glasses?\n",
      "right\n",
      "right\n",
      "Variable containing:\n",
      " 23.7053\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "right\n",
      "Variable containing:\n",
      " 21.5865\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "photo shoot\n",
      "Variable containing:\n",
      " 2.1617\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://images.cocodataset.org/train2014/COCO_train2014_000000376241.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where is the guy ?\n",
      "\n",
      "sunbathing\n",
      "Variable containing:\n",
      " 21.4228\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "sunbathing\n",
      "Variable containing:\n",
      " 20.9715\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "wii remote\n",
      "Variable containing:\n",
      " 1.3998\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://images.cocodataset.org/train2014/COCO_train2014_000000434045.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the person doing?\n",
      "sunbathing\n",
      "sunbathing\n",
      "Variable containing:\n",
      " 21.2756\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "sunbathing\n",
      "Variable containing:\n",
      " 20.9715\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "wii remote\n",
      "Variable containing:\n",
      " 2.0326\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://images.cocodataset.org/train2014/COCO_train2014_000000434045.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where is the guy ?\n",
      "\n",
      "sunny\n",
      "Variable containing:\n",
      " 18.5967\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "sunny\n",
      "Variable containing:\n",
      " 18.0189\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "wii remote\n",
      "Variable containing:\n",
      " 1.3998\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://images.cocodataset.org/train2014/COCO_train2014_000000167330.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How does the weather appear in this photo?\n",
      "sunny\n",
      "sunny\n",
      "Variable containing:\n",
      " 20.9351\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "sunny\n",
      "Variable containing:\n",
      " 18.0189\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "department\n",
      "Variable containing:\n",
      " 3.0770\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://images.cocodataset.org/train2014/COCO_train2014_000000167330.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where is the guy ?\n",
      "\n",
      "greenhouse\n",
      "Variable containing:\n",
      " 22.4208\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "greenhouse\n",
      "Variable containing:\n",
      " 22.1409\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "wii remote\n",
      "Variable containing:\n",
      " 1.3998\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://images.cocodataset.org/train2014/COCO_train2014_000000158225.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What kind of facility are the people standing in?\n",
      "greenhouse\n",
      "greenhouse\n",
      "Variable containing:\n",
      " 23.3049\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "greenhouse\n",
      "Variable containing:\n",
      " 22.1409\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "wii remote\n",
      "Variable containing:\n",
      " 2.0801\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://images.cocodataset.org/train2014/COCO_train2014_000000158225.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where is the guy ?\n",
      "\n",
      "octagon\n",
      "Variable containing:\n",
      " 20.2347\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "octagon\n",
      "Variable containing:\n",
      " 19.5222\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "wii remote\n",
      "Variable containing:\n",
      " 1.3998\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://images.cocodataset.org/train2014/COCO_train2014_000000113236.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What shape is this?\n",
      "octagon\n",
      "octagon\n",
      "Variable containing:\n",
      " 20.7003\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "octagon\n",
      "Variable containing:\n",
      " 19.5222\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "nothing\n",
      "Variable containing:\n",
      " 1.7935\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://images.cocodataset.org/train2014/COCO_train2014_000000113236.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where is the guy ?\n",
      "\n",
      "red\n",
      "Variable containing:\n",
      " 24.3147\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "red\n",
      "Variable containing:\n",
      " 24.1037\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "wii remote\n",
      "Variable containing:\n",
      " 1.3998\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://images.cocodataset.org/train2014/COCO_train2014_000000277284.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What color is the Frisbee in the man's hand?\n",
      "red\n",
      "red\n",
      "Variable containing:\n",
      " 25.3193\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "red\n",
      "Variable containing:\n",
      " 24.1037\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "wii remote\n",
      "Variable containing:\n",
      " 2.7175\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://images.cocodataset.org/train2014/COCO_train2014_000000277284.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where is the guy ?\n",
      "\n",
      "motorcycle\n",
      "Variable containing:\n",
      " 20.4001\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "motorcycle\n",
      "Variable containing:\n",
      " 19.8033\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "wii remote\n",
      "Variable containing:\n",
      " 1.3998\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://images.cocodataset.org/train2014/COCO_train2014_000000496740.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is this person riding?\n",
      "motorcycle\n",
      "motorcycle\n",
      "Variable containing:\n",
      " 21.1104\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "motorcycle\n",
      "Variable containing:\n",
      " 19.8033\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "nothing\n",
      "Variable containing:\n",
      " 2.0364\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://images.cocodataset.org/train2014/COCO_train2014_000000496740.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where is the guy ?\n",
      "\n",
      "brown\n",
      "Variable containing:\n",
      " 21.9165\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "brown\n",
      "Variable containing:\n",
      " 21.6731\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "wii remote\n",
      "Variable containing:\n",
      " 1.3998\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://images.cocodataset.org/train2014/COCO_train2014_000000143959.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What color are the frames of the glasses?\n",
      "brown\n",
      "brown\n",
      "Variable containing:\n",
      " 24.0972\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "brown\n",
      "Variable containing:\n",
      " 21.6731\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "department\n",
      "Variable containing:\n",
      " 3.0988\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://images.cocodataset.org/train2014/COCO_train2014_000000143959.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "def show_predict(idx, question = None):\n",
    "    #idx = 1\n",
    "    \n",
    "    if question == None:       \n",
    "        ans = adata_train['annotations'][idx]['multiple_choice_answer']\n",
    "    else:\n",
    "        ans = ''\n",
    "        \n",
    "    if question == None:\n",
    "        question = qdata_train['questions'][idx]['question']\n",
    "    img_id =  qdata_train['questions'][idx]['image_id']\n",
    "    \n",
    "    print(question)\n",
    "    print(ans)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    with open(DATA_PATH + 'imgid2imginfo.json', 'r') as file:\n",
    "        imgid2info = json.load(file)\n",
    "\n",
    "\n",
    "    #question_word_vector = calc_scores(list(sent2i(question))[0]).data.numpy().reshape(65,)\n",
    "\n",
    "    question_word_vector = None\n",
    "    question_split = question.split(' ')\n",
    "    for question_word in question_split:            \n",
    "        if question_word not in w2v_model:                                \n",
    "            question_word = question_word[:-1]\n",
    "            if question_word not in w2v_model:\n",
    "                #print(question_word)\n",
    "                continue\n",
    "        if question_word_vector is None:\n",
    "            question_word_vector = np.array(w2v_model[question_word])\n",
    "        else:\n",
    "            question_word_vector += np.array(w2v_model[question_word])\n",
    "    \n",
    "            #get image vector\n",
    "    h5_id = visual_feat_mapping[str(img_id)]\n",
    "    img_feat = img_features[h5_id]\n",
    "    #print(img_feat)\n",
    "            #concatenate word vecotr and image vector\n",
    "    img_word_vector = np.concatenate((question_word_vector, img_feat), axis=0)\n",
    "    x = Variable(torch.from_numpy(img_word_vector).type(torch.FloatTensor))#.cuda()\n",
    "    predict_y = model(x)        \n",
    "    print(ans_soft_max[predict_y.data.numpy().argmax()])\n",
    "    print(predict_y.max())\n",
    "   \n",
    "    zero_question_word_vector = np.zeros(300,)\n",
    "    img_word_vector = np.concatenate((zero_question_word_vector, img_feat), axis=0)\n",
    "    x_img = Variable(torch.from_numpy(img_word_vector).type(torch.FloatTensor))#.cuda()\n",
    "    predict_y_img = model(x_img)\n",
    "    print(ans_soft_max[predict_y_img.data.numpy().argmax()])\n",
    "    print(predict_y_img.max())\n",
    "    \n",
    "    zero_question_img_vector = np.zeros(2048,)\n",
    "    img_word_vector = np.concatenate((question_word_vector, zero_question_img_vector), axis=0)\n",
    "    x_word = Variable(torch.from_numpy(img_word_vector).type(torch.FloatTensor))#.cuda()\n",
    "    predict_y_word = model(x_word)\n",
    "    print(ans_soft_max[predict_y_word.data.numpy().argmax()])\n",
    "    print(predict_y_word.max())\n",
    "\n",
    "    display(Image(url= imgid2info[str(img_id)]['coco_url']))\n",
    "for i in range(10):\n",
    "    show_predict(i, 'where is the guy ?')\n",
    "    show_predict(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
