{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Creation Looks good! Time Taken 1.50\n",
      "Training Set Size: 48061\n",
      "\n",
      "Validation Set Size: 8977\n",
      "\n",
      "Test Set Size: 2962\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import zipfile\n",
    "import random\n",
    "import numpy as np\n",
    "import h5py\n",
    "from collections import Counter, defaultdict\n",
    "from time import time\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import gzip\n",
    "import os\n",
    "import torch.optim as optim\n",
    "\n",
    "#only using cbow + MLP \n",
    "USING_CBOW = False\n",
    "#use (CBOW -> LSTM) + MLP\n",
    "LSTM_TRAINED_WITH_CBOW_EMBEDDINGS = True\n",
    "UPDATE_CBOW_EMBEDDING = True\n",
    "\n",
    "DATA_PATH = '..' + os.sep + 'NLP1-2017-VQA' + os.sep + 'data' + os.sep\n",
    "\n",
    "with zipfile.ZipFile('./data/v2_Questions_Train_mscoco.zip', 'r') as file:\n",
    "    qdata = json.load(file.open(file.namelist()[0]))\n",
    "\n",
    "with zipfile.ZipFile('./data/v2_Annotations_Train_mscoco.zip', 'r') as file:\n",
    "    adata = json.load(file.open(file.namelist()[0])) \n",
    "    \n",
    "#statistics\n",
    "question_types = set()\n",
    "multiple_choice_answers = set()\n",
    "answer2count = defaultdict(int)\n",
    "answer_types = set()\n",
    "answertypes2count = defaultdict(int)\n",
    "top_answers_per_type = defaultdict(lambda: defaultdict(int))\n",
    "for ann in adata['annotations']:\n",
    "    question_types.add(ann['question_type'])\n",
    "    \n",
    "    multiple_choice_answers.add(ann['multiple_choice_answer'])\n",
    "    \n",
    "    answer2count[ann['multiple_choice_answer']] += 1\n",
    "    answer_types.add(ann['answer_type'])\n",
    "    \n",
    "    answertypes2count[ann['answer_type']] += 1\n",
    "    top_answers_per_type[ann['answer_type']][ann['multiple_choice_answer']] += 1\n",
    "\n",
    "    \n",
    "#dataset creation\n",
    "start_time = time()\n",
    "idx = list(range(0,len(qdata['questions'])))\n",
    "random.seed(42)\n",
    "random.shuffle(idx)\n",
    "\n",
    "np.random.seed(42)\n",
    "splits = ['train', 'valid', 'test']\n",
    "\n",
    "n = 20000\n",
    "qdata_small = {'questions': list()}\n",
    "adata_small = {'annotations': list()}\n",
    "a_type_counts = {'yes/no': 0, 'number': 0, 'other': 0}\n",
    "\n",
    "while len(qdata_small['questions']) < 3*n:\n",
    "    i = idx.pop()\n",
    "    \n",
    "    at = adata['annotations'][i]['answer_type'] \n",
    "    \n",
    "    if a_type_counts[at] < n:\n",
    "        \n",
    "        if at == 'yes/no' and adata['annotations'][i]['multiple_choice_answer'] not in ['yes', 'no']:\n",
    "            continue\n",
    "            \n",
    "        adata_small['annotations'].append(adata['annotations'][i])\n",
    "        qdata_small['questions'].append(qdata['questions'][i])\n",
    "        \n",
    "        split = np.random.choice(splits, p=(.8, .15, .05))\n",
    "        adata_small['annotations'][-1]['split'] = split\n",
    "        qdata_small['questions'][-1]['split'] = split\n",
    "        \n",
    "        a_type_counts[at] += 1\n",
    "        \n",
    "# Tests\n",
    "assert len(qdata_small['questions']) == len(adata_small['annotations']) == 3*n, \"Inconsitent Lengths.\"\n",
    "a_type_counts = {'yes/no': 0, 'number': 0, 'other': 0}\n",
    "for ann in adata_small['annotations']:\n",
    "    a_type_counts[ann['answer_type']] += 1\n",
    "assert a_type_counts['yes/no'] == a_type_counts['number'] == a_type_counts['other'] == n, \"Inconsistent Answer Type Lengths.\"\n",
    "\n",
    "print(\"Data Creation Looks good! Time Taken %.2f\" %(time()-start_time))\n",
    "\n",
    "\n",
    "question_types = set()\n",
    "multiple_choice_answers = set()\n",
    "answer2count = defaultdict(int)\n",
    "answer_types = set()\n",
    "answertypes2count = defaultdict(int)\n",
    "top_answers_per_type = defaultdict(lambda: defaultdict(int))\n",
    "for ann in adata_small['annotations']:\n",
    "    question_types.add(ann['question_type'])\n",
    "    \n",
    "    multiple_choice_answers.add(ann['multiple_choice_answer'])\n",
    "    \n",
    "    answer2count[ann['multiple_choice_answer']] += 1\n",
    "    answer_types.add(ann['answer_type'])\n",
    "    \n",
    "    answertypes2count[ann['answer_type']] += 1\n",
    "    top_answers_per_type[ann['answer_type']][ann['multiple_choice_answer']] += 1\n",
    "\n",
    "\n",
    "#saving\n",
    "qdata_small_splits = {\\\n",
    "                      'train': {'questions': list()}, \n",
    "                      'valid': {'questions': list()}, \n",
    "                      'test': {'questions': list()}\n",
    "                     }\n",
    "\n",
    "adata_small_splits = {\\\n",
    "                      'train': {'annotations': list()}, \n",
    "                      'valid': {'annotations': list()}, \n",
    "                      'test': {'annotations': list()}\n",
    "                     }\n",
    "\n",
    "for i in range(len(qdata_small['questions'])):\n",
    "    \n",
    "    split = qdata_small['questions'][i]['split']\n",
    "    assert split == adata_small['annotations'][i]['split'], \"Inconsistent Splits.\"\n",
    "    assert adata_small['annotations'][i]['question_id'] == qdata_small['questions'][i]['question_id'], \"Inconsistent IDs.\"\n",
    "    \n",
    "    qdata_small_splits[split]['questions'].append(qdata_small['questions'][i])\n",
    "    adata_small_splits[split]['annotations'].append(adata_small['annotations'][i])\n",
    "    \n",
    "        \n",
    "print(\"Training Set Size: %i\" %(len(qdata_small_splits['train']['questions'])))\n",
    "print(\"\\nValidation Set Size: %i\" %(len(qdata_small_splits['valid']['questions'])))\n",
    "print(\"\\nTest Set Size: %i\" %(len(qdata_small_splits['test']['questions'])))\n",
    "\n",
    "\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    \n",
    "    with gzip.GzipFile('data/vqa_annotatons_' + split + '.gzip', 'w') as file:\n",
    "        file.write(json.dumps(adata_small_splits[split]).encode('utf-8'))\n",
    "        \n",
    "    with gzip.GzipFile('data/vqa_questions_' + split + '.gzip', 'w') as file:\n",
    "        file.write(json.dumps(qdata_small_splits[split]).encode('utf-8'))\n",
    "\n",
    "image_ids = set()\n",
    "for q in qdata_small['questions']:\n",
    "    image_ids.add(q['image_id'])\n",
    "\n",
    "image_ids_json = {'image_ids': list(image_ids)}\n",
    "with open('data/image_ids_vqa.json', 'w') as file:\n",
    "    json.dump(image_ids_json, file)\n",
    "    \n",
    "    \n",
    "#read data\n",
    "with gzip.open(DATA_PATH + 'vqa_questions_train.gzip', 'rb') as file:\n",
    "    file_content = file.read().decode('utf-8')\n",
    "    qdata_train = json.loads(file_content)\n",
    "\n",
    "with gzip.GzipFile(DATA_PATH + 'vqa_annotatons_train.gzip', 'r') as file:\n",
    "    adata_train = json.loads(file.read().decode('utf-8')) \n",
    "    \n",
    "with gzip.open(DATA_PATH + 'vqa_questions_test.gzip', 'rb') as file:\n",
    "    file_content = file.read().decode('utf-8')\n",
    "    qdata_test = json.loads(file_content)\n",
    "    \n",
    "with gzip.GzipFile(DATA_PATH + 'vqa_annotatons_test.gzip', 'r') as file:\n",
    "    adata_test = json.loads(file.read().decode('utf-8')) \n",
    "    \n",
    "#print(qdata_train['questions'][0])\n",
    "#print(\"#1: \", adata_train['annotations'][0])\n",
    "\n",
    "question_types = set()\n",
    "multiple_choice_answers = set()\n",
    "answer2count = defaultdict(int)\n",
    "answer_types = set()\n",
    "answertypes2count = defaultdict(int)\n",
    "top_answers_per_type = defaultdict(lambda: defaultdict(int))\n",
    "for ann in adata_train['annotations']:\n",
    "    question_types.add(ann['question_type'])\n",
    "    \n",
    "    multiple_choice_answers.add(ann['multiple_choice_answer'])\n",
    "    \n",
    "    answer2count[ann['multiple_choice_answer']] += 1\n",
    "    answer_types.add(ann['answer_type'])\n",
    "    \n",
    "    answertypes2count[ann['answer_type']] += 1\n",
    "    top_answers_per_type[ann['answer_type']][ann['multiple_choice_answer']] += 1    \n",
    "    \n",
    "    \n",
    "torch.manual_seed(1)\n",
    "\n",
    "#w2i is the dict that change vocabulary used in question to index, exmpale: how are you -> {1,3,4}\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "\n",
    "w2i['pad'] = 0\n",
    "UNK = w2i[\"<unk>\"]\n",
    "\n",
    "#answer to index dictionary\n",
    "ans_dict = {'pad': 0, '<unk>': 1}\n",
    "#ans_dict = {'unk': 0}\n",
    "\n",
    "#index to answer dictionary\n",
    "rev_ans_dict = {'0': 'pad', '1': '<unk>'}\n",
    "#rev_ans_dict = {'0': 'unk'}\n",
    "\n",
    "#answer which occur more than 5 times, then it will be add to answer dictionary\n",
    "def answer_to_idx(answer2count):\n",
    "    count = 1\n",
    "    for ans in answer2count: \n",
    "        if answer2count[ans] > 5:\n",
    "            ans_dict[ans] = count\n",
    "            rev_ans_dict[str(count)] = ans\n",
    "            count = count + 1\n",
    "    return ans_dict\n",
    "\n",
    "#also include 'unk here', not sure if it's right???\n",
    "def ans_to_onehot(ans_idx):\n",
    "    #ans_idx is the integer index of an answer\n",
    "    ans1h = np.zeros(no_ans).reshape(no_ans)\n",
    "    ans1h[ans_idx] = 1\n",
    "    return ans1h\n",
    "\n",
    "\n",
    "#change vocabular in question sets to index\n",
    "#it will return a list with every words in question changing to index and also the anser index\n",
    "def read_data_with_answers(t_data, t_answer_data, training ):\n",
    "    for idx in range(len(t_data)):\n",
    "        question = t_data[idx]['question']\n",
    "        question = question.lower().split(\"?\",1)[0]\n",
    "        \n",
    "        question_with_pad = question.split(\" \")\n",
    "        question_lenth = len(question_with_pad)\n",
    "        \n",
    "        #adding pad token to the question until the length of the sentence is = 10\n",
    "        for idx_1 in range(10-question_lenth):\n",
    "            question_with_pad.append('pad')        \n",
    "            \n",
    "        ans_yield = 0\n",
    "        \n",
    "        if t_answer_data[idx]['multiple_choice_answer'] in ans_dict:\n",
    "            ans_yield = ans_dict[t_answer_data[idx]['multiple_choice_answer']]\n",
    "        \n",
    "        if training == True:\n",
    "            yield ([w2i[x] for x in question_with_pad], ans_yield)   \n",
    "        else:\n",
    "            yield ([w2i[x] if x in w2i else w2i[\"<unk>\"] for x in question_with_pad ], ans_yield)\n",
    "             \n",
    "\n",
    "\n",
    "#answer vocabulary \n",
    "ans_dict = answer_to_idx(answer2count)\n",
    "\n",
    "#number of answers which occur more than 5 times in training set\n",
    "no_ans = len(ans_dict)\n",
    "\n",
    "data_ans_train = list(read_data_with_answers(qdata_train['questions'], adata_train['annotations'], training = True))\n",
    "w2i = defaultdict(lambda: UNK, w2i)\n",
    "data_ans_test = list(read_data_with_answers(qdata_test['questions'], adata_test['annotations'], training = False))\n",
    "\n",
    "nwords = len(w2i)\n",
    "    \n",
    "#after training, get word embedding layer through cbow, input should be a question_to_index_list(w2i?)\n",
    "def get_word_embedding_layer(data):\n",
    "    \n",
    "    #get bow vector from each question, so question_bow will be a matrix \n",
    "    #each row of matrix is a vector coming from sum of word embeddings from each questions\n",
    "    question_bow = np.zeros((len(data), no_embeddings))\n",
    "    \n",
    "    #iterate each question, so we can append each bow of question to question_bow\n",
    "    for idx, words in enumerate(data):\n",
    "        lookup_tensor = Variable(torch.LongTensor([words]))\n",
    "        scores = model(lookup_tensor)\n",
    "        question_bow[idx] = (model.bows.data.numpy())\n",
    "    \n",
    "    return question_bow#FOR TRAINING on UNIQUE WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW (\n",
      "  (embeddings): Embedding(6941, 1024, padding_idx=0)\n",
      "  (linear): Linear (1024 -> 532)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "no_embeddings = 1024   \n",
    "all_losses = []\n",
    "\n",
    "if LSTM_TRAINED_WITH_CBOW_EMBEDDINGS or USING_CBOW:\n",
    "    \n",
    "    class CBOW(nn.Module):\n",
    "        \n",
    "        def __init__(self, vocab_size, embedding_dim, output_dim):\n",
    "            super(CBOW, self).__init__()\n",
    "            self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx = 0)\n",
    "            self.linear = nn.Linear(embedding_dim, output_dim)\n",
    "            self.bows = torch.FloatTensor(1, embedding_dim).zero_()\n",
    "\n",
    "        def forward(self, inputs):\n",
    "            embeds = self.embeddings(inputs)\n",
    "            #print(embeds.size)\n",
    "            bow = torch.sum(embeds, 1)\n",
    "            self.bows = bow\n",
    "            logits = self.linear(bow)\n",
    "            return logits\n",
    "\n",
    "\n",
    "    model = CBOW(nwords, no_embeddings, no_ans)\n",
    "    print(model)\n",
    "\n",
    "\n",
    "    def evaluate(model, data):\n",
    "        \"\"\"Evaluate a model on a data set.\"\"\"\n",
    "        correct = 0.0\n",
    " \n",
    "        for words, tag in data:\n",
    "            lookup_tensor = Variable(torch.LongTensor([words]))\n",
    "            scores = model(lookup_tensor)\n",
    "            predict = scores.data.numpy().argmax(axis=1)[0]\n",
    "\n",
    "            if predict == tag:\n",
    "                correct += 1\n",
    "\n",
    "        return correct, len(data), correct/len(data), scores\n",
    "\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    for ITER in range(50):\n",
    "\n",
    "        #random.shuffle(data_ans_train)\n",
    "        train_loss = 0.0\n",
    "        start = time()\n",
    "        correct = 0\n",
    "        \n",
    "        for words, tag in data_ans_train:\n",
    "            # forward pass\n",
    "            lookup_tensor = Variable(torch.LongTensor([words]))\n",
    "            scores = model(lookup_tensor)\n",
    "            loss = nn.CrossEntropyLoss()\n",
    "            target = Variable(torch.LongTensor([tag]))\n",
    "            \n",
    "            predict = scores.data.numpy().argmax(axis=1)[0]\n",
    "            if predict == tag:\n",
    "                correct += 1\n",
    "            \n",
    "            \n",
    "            output = loss(scores, target)\n",
    "            train_loss += output.data[0]\n",
    "\n",
    "            # backward pass\n",
    "            model.zero_grad()\n",
    "            output.backward()\n",
    "\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"iter %r: train loss/sent=%.4f, time=%.2fs\" % \n",
    "              (ITER, train_loss/len(data_ans_train), time()-start))\n",
    "        print('correct', correct/len(data_ans_train))\n",
    "\n",
    "        all_losses.append(1 - correct/len(data_ans_train))\n",
    "# if iter % plot_every == 0:\n",
    "#         all_losses.append(current_loss / plot_every)\n",
    "#         current_loss = 0  \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#after training, get word embedding layer through cbow, input should be a question_to_index_list(w2i?)\n",
    "def get_word_embedding_layer(data):\n",
    "    \n",
    "    #get bow vector from each question, so question_bow will be a matrix \n",
    "    #each row of matrix is a vector coming from sum of word embeddings from each questions\n",
    "    question_bow = np.zeros((len(data), no_embeddings))\n",
    "    \n",
    "    #iterate each question, so we can append each bow of question to question_bow\n",
    "    for idx, words in enumerate(data):\n",
    "        lookup_tensor = Variable(torch.LongTensor([words]))\n",
    "        scores = model(lookup_tensor)\n",
    "        question_bow[idx] = (model.bows.data.numpy())\n",
    "    \n",
    "    return question_bow\n",
    "\n",
    "#test, get_word_embedding_layer get correct answer   \n",
    "#print((get_word_embedding_layer(list([data_ans_train[0][0]]))).shape)   #size 1x64\n",
    "\n",
    "#read from image\n",
    "path_to_h5_file = DATA_PATH + 'VQA_image_features.h5'\n",
    "path_to_json_file = DATA_PATH + 'VQA_img_features2id.json'\n",
    "\n",
    "#get image feature from h5_id \n",
    "img_features = np.asarray(h5py.File(path_to_h5_file, 'r')['img_features'])\n",
    "\n",
    "#get h5_id from image_id, which can see in the answer data \n",
    "with open(path_to_json_file, 'r') as f:\n",
    "     visual_feat_mapping = json.load(f)['VQA_imgid2id']\n",
    "\n",
    "        \n",
    "# print(model.embeddings)\n",
    "# print(model.embeddings.weight.data)\n",
    "# print(type(model.embeddings.weight.data))\n",
    "# word_embeddings = nn.Embedding(6941, 64, padding_idx = 0)\n",
    "# word_embeddings.weight = nn.Parameter(model.embeddings.weight.data)\n",
    "\n",
    "# print(word_embeddings)    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not USING_CBOW:\n",
    "\n",
    "    class LSTMTagger(nn.Module):\n",
    "\n",
    "        def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "            super(LSTMTagger, self).__init__()\n",
    "            self.hidden_dim = hidden_dim\n",
    "\n",
    "            if not LSTM_TRAINED_WITH_CBOW_EMBEDDINGS:\n",
    "                self.word_embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx = 0)\n",
    "    \n",
    "            if LSTM_TRAINED_WITH_CBOW_EMBEDDINGS and UPDATE_CBOW_EMBEDDING:\n",
    "                self.word_embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx = 0)\n",
    "                self.word_embeddings.weight = nn.Parameter(model.embeddings.weight.data)\n",
    "                          \n",
    "\n",
    "            # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "            # with dimensionality hidden_dim.\n",
    "            self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "            # The linear layer that maps from hidden state space to tag space\n",
    "            self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "            self.hidden = self.init_hidden()\n",
    "\n",
    "        def init_hidden(self):\n",
    "            # Before we've done anything, we dont have any hidden state.\n",
    "            # Refer to the Pytorch documentation to see exactly\n",
    "            # why they have this dimensionality.\n",
    "            # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "            return (torch.autograd.Variable(torch.zeros(1, 1, self.hidden_dim)),\n",
    "                    torch.autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
    "\n",
    "        def forward(self, sentence):\n",
    "\n",
    "            if LSTM_TRAINED_WITH_CBOW_EMBEDDINGS:\n",
    "                embeds = model.embeddings(sentence)\n",
    "            else:\n",
    "                embeds = self.word_embeddings(sentence)\n",
    "\n",
    "            lstm_out, self.hidden = self.lstm(\n",
    "                embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "            tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "            tag_scores = nn.functional.log_softmax(tag_space)\n",
    "            return tag_scores\n",
    "\n",
    "    LSTMmodel = LSTMTagger(no_embeddings, no_embeddings, nwords, no_ans)\n",
    "    loss_function = nn.NLLLoss()\n",
    "    #optimizer = optim.SGD(LSTMmodel.parameters(), lr=0.05)\n",
    "    \n",
    "    if UPDATE_CBOW_EMBEDDING:\n",
    "            \n",
    "        optimizer = optim.SGD([\n",
    "                    {'params': LSTMmodel.word_embeddings.parameters(), 'lr': 1e-5},\n",
    "                    {'params': LSTMmodel.lstm.parameters(), 'lr' : 1e-3},\n",
    "                    {'params': LSTMmodel.hidden2tag.parameters(),'lr' : 1e-3}\n",
    "                ], momentum=0.9)\n",
    "\n",
    "    elif LSTM_TRAINED_WITH_CBOW_EMBEDDINGS:\n",
    "        \n",
    "        optimizer = optim.SGD([\n",
    "                    {'params': LSTMmodel.lstm.parameters(), 'lr' : 1e-3},\n",
    "                    {'params': LSTMmodel.hidden2tag.parameters(),'lr' : 1e-3}\n",
    "                ], momentum=0.9)\n",
    "    else:\n",
    "        optimizer = optim.SGD(LSTMmodel.parameters(), lr=0.05)\n",
    "        \n",
    "\n",
    "    for epoch in range(1000):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "\n",
    "        #random.shuffle(data_ans_train)\n",
    "        train_loss = 0.0\n",
    "        start = time.time()\n",
    "        correct = 0.0\n",
    "        #for words, tag in data_ans_train[0:10]:\n",
    "        for random_idx in range(5000):        \n",
    "\n",
    "            #shuffle data\n",
    "            word_idx = random.randint(0,len(data_ans_train)-1)\n",
    "            words = data_ans_train[word_idx][0]\n",
    "            tag = data_ans_train[word_idx][1]\n",
    "\n",
    "            # Step 1. Remember that Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            LSTMmodel.zero_grad()\n",
    "\n",
    "            # Also, we need to clear out the hidden state of the LSTM,\n",
    "            # detaching it from its history on the last instance.\n",
    "            LSTMmodel.hidden = LSTMmodel.init_hidden()\n",
    "\n",
    "            # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "            # Variables of word indices.\n",
    "            try:\n",
    "                expand_length = words.index(0)\n",
    "                words = words[0:words.index(0)]\n",
    "            except ValueError as e:\n",
    "                words = words[0:10]\n",
    "                expand_length = 10\n",
    "\n",
    "            lookup_tensor = Variable(torch.LongTensor([words])).view(-1) \n",
    "            targets = Variable(torch.LongTensor([tag])).expand(expand_length)\n",
    "\n",
    "            # Step 3. Run our forward pass.\n",
    "            tag_scores = LSTMmodel(lookup_tensor)#.view(1, 10, no_ans)\n",
    "            tag_predict = tag_scores.data.numpy()[-1].argmax()\n",
    "\n",
    "            if (tag_predict == tag):\n",
    "                correct += 1\n",
    "\n",
    "\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            loss.backward()\n",
    "            train_loss += loss.data[0]\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        if (correct/5000 > 0.85):\n",
    "            break    \n",
    "\n",
    "        if (epoch % 2 == 0):\n",
    "            print(\"iter %r: train loss/sent=%.4f, time=%.2fs\" % \n",
    "                      (epoch, train_loss/5000, time.time()-start))\n",
    "            print('correct:', correct/5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test LSTM result\n",
    "if not USING_CBOW:\n",
    "    for words, tag in data_ans_train[0:8]:\n",
    "\n",
    "        LSTMmodel.hidden = LSTMmodel.init_hidden()\n",
    "        try:\n",
    "            expand_length = words.index(0)\n",
    "            words = words[0:words.index(0)]\n",
    "        except ValueError as e:\n",
    "            words = words[0:10]\n",
    "            expand_length = 10\n",
    "\n",
    "        lookup_tensor1 = Variable(torch.LongTensor([words])).view(-1) \n",
    "        targets = Variable(torch.LongTensor([tag])).expand(expand_length)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = LSTMmodel(lookup_tensor1)#.view(1, 10, no_ans)\n",
    "        tag_predict = tag_scores.data.numpy()[-1].argmax()\n",
    "        print(tag_predict, tag)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build MLP network\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer\n",
    "        self.out = torch.nn.Linear(n_hidden, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.hidden(x))      # activation function for hidden layer\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if USING_CBOW:\n",
    "    n_input = img_feat.shape[0] + no_embeddings\n",
    "else:\n",
    "    n_input = img_feat.shape[0] + no_ans\n",
    "\n",
    "n_output = len(ans_dict)\n",
    "n_hidden_size = 200\n",
    "learning_rate = 0.001\n",
    "\n",
    "word_img_model = Net(n_input, n_hidden_size, n_output)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(word_img_model.parameters(), lr=learning_rate)\n",
    "\n",
    "temp_x = Variable()\n",
    "temp_y = Variable()\n",
    "\n",
    "for ITER in range(100):\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    #loss = []\n",
    "    #print('start!')\n",
    "    count_err = 0\n",
    "    #for idx, adata in enumerate(adata_train['annotations'][0:3]):\n",
    "    for random_idx in range(5000):\n",
    "        \n",
    "        idx = random.randint(0,len(data_ans_train)-1)\n",
    "        adata = adata_train['annotations'][idx]\n",
    "                \n",
    "        #get text vector:\n",
    "        if USING_CBOW:\n",
    "            #try to use cbow:\n",
    "            question_word_vector = get_word_embedding_layer(list([data_ans_train[idx][0]])).reshape( no_embeddings, ) \n",
    "        else:\n",
    "            #try to use LSTM        \n",
    "            words = data_ans_train[idx][0]\n",
    "            tag = data_ans_train[idx][1]\n",
    "            LSTMmodel.hidden = LSTMmodel.init_hidden()\n",
    "            try:\n",
    "                expand_length = words.index(0)\n",
    "                words = words[0:words.index(0)]\n",
    "            except ValueError as e:\n",
    "                words = words[0:10]\n",
    "                expand_length = 10\n",
    "\n",
    "            lookup_tensor1 = Variable(torch.LongTensor([words])).view(-1) \n",
    "            targets = Variable(torch.LongTensor([tag])).expand(expand_length)\n",
    "\n",
    "            # Step 3. Run our forward pass.\n",
    "            tag_scores = LSTMmodel(lookup_tensor1)#.view(1, 10, no_ans)\n",
    "            question_word_vector = tag_scores.data.numpy()[-1]\n",
    "\n",
    "            #LSTM word vecotr normalize\n",
    "            question_word_vector = (question_word_vector - question_word_vector.mean()) / (np.max(question_word_vector) - np.min(question_word_vector))\n",
    "\n",
    "        #get image vector\n",
    "        h5_id = visual_feat_mapping[str(adata['image_id'])]        \n",
    "        img_feat = img_features[h5_id]\n",
    "        \n",
    "        #concatenate word vecotr and image vector : (64,)+(2048,) = (2112,)    \n",
    "        img_word_vector = np.concatenate((question_word_vector, img_feat), axis=0)\n",
    "        \n",
    "        #get answer's word index\n",
    "        answer_index = 0 if not(adata['multiple_choice_answer'] in ans_dict) else ans_dict[adata['multiple_choice_answer']]\n",
    "        \n",
    "        output_vector = np.array([answer_index])#.reshape(1,1)\n",
    "        x = Variable(torch.from_numpy(img_word_vector).type(torch.FloatTensor))#.cuda()\n",
    "        y = Variable(torch.from_numpy(output_vector))#.cuda()        \n",
    "        \n",
    "        y_pred = word_img_model(x).view(1,n_output)\n",
    "        if (ITER  % 5 == 0) and (random_idx < 5):\n",
    "            print(rev_ans_dict[str(y_pred.data.numpy().argmax())], adata['multiple_choice_answer'])\n",
    "        \n",
    "        if rev_ans_dict[str(y_pred.data.numpy().argmax())] != adata['multiple_choice_answer']:\n",
    "            count_err += 1\n",
    "        #if idx % 1000 == 0:\n",
    "        #    print('idx ' + str(idx))\n",
    "        loss = loss_fn(y_pred, y)\n",
    "            \n",
    "        train_loss += loss.data[0]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()   \n",
    "    \n",
    "    \n",
    "    if (ITER  % 2 == 0):\n",
    "        print('{:>5}'.format(ITER),' loss: ', train_loss/5000)#len(adata_train['annotations']))\n",
    "        print(ITER,' err: ', count_err/5000)#len(adata_train['annotations']))\n",
    "        #pass\n",
    "        #print('{:>5}'.format(ITER),' loss: ', train_loss)\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "with open('./data/imgid2imginfo.json', 'r') as file:\n",
    "    imgid2info = json.load(file)\n",
    "\n",
    "def show_predict(idx, question = None):\n",
    "    #idx = 1\n",
    "    \n",
    "    if question == None:       \n",
    "        ans = adata_train['annotations'][idx]['multiple_choice_answer']\n",
    "    else:\n",
    "        ans = ''\n",
    "        \n",
    "    if question == None:\n",
    "        question = qdata_train['questions'][idx]['question']\n",
    "    img_id =  qdata_train['questions'][idx]['image_id']\n",
    "    \n",
    "    print(question)\n",
    "    print('real answer: ',ans)\n",
    "    \n",
    "    #print(w2i['question'])\n",
    "    \n",
    "    if USING_CBOW:\n",
    "        #CBOW word vector\n",
    "        word_idx = []\n",
    "        question_split = question.split(' ')\n",
    "        for word in question_split:\n",
    "            word_idx += [w2i[word]]\n",
    "            \n",
    "        question_word_vector = get_word_embedding_layer([word_idx]).reshape( no_embeddings, ) \n",
    "    else:\n",
    "        words = data_ans_train[idx][0]\n",
    "        tag = data_ans_train[idx][1]\n",
    "        LSTMmodel.hidden = LSTMmodel.init_hidden()\n",
    "        try:\n",
    "            expand_length = words.index(0)\n",
    "            words = words[0:words.index(0)]\n",
    "        except ValueError as e:\n",
    "            words = words[0:10]\n",
    "            expand_length = 10\n",
    "\n",
    "        lookup_tensor1 = Variable(torch.LongTensor([words])).view(-1) \n",
    "        targets = Variable(torch.LongTensor([tag])).expand(expand_length)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = LSTMmodel(lookup_tensor1)#.view(1, 10, no_ans)\n",
    "        question_word_vector = tag_scores.data.numpy()[-1]\n",
    "\n",
    "        #LSTM word vecotr normalize\n",
    "        question_word_vector = (question_word_vector - question_word_vector.mean()) / (np.max(question_word_vector) - np.min(question_word_vector))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #get image vector\n",
    "    h5_id = visual_feat_mapping[str(img_id)]\n",
    "    img_feat = img_features[h5_id]\n",
    "    #print(img_feat)tag_scores\n",
    "    #concatenate word vecotr and image vector\n",
    "    img_word_vector = np.concatenate((question_word_vector, img_feat), axis=0)\n",
    "    print(img_word_vector.shape, question_word_vector.shape, img_feat.shape)\n",
    "    #predict the output\n",
    "    x = Variable(torch.from_numpy(img_word_vector).type(torch.FloatTensor))\n",
    "    predict_y = word_img_model(x)        \n",
    "    print('model prediction: ', rev_ans_dict[str(predict_y.data.numpy().argmax())])\n",
    "    #print(predict_y.max())\n",
    "    '''\n",
    "    #using image vector to predict the output\n",
    "    zero_question_word_vector = np.zeros(no_embeddings,)\n",
    "    img_word_vector = np.concatenate((zero_question_word_vector, img_feat), axis=0)\n",
    "    x_img = Variable(torch.from_numpy(img_word_vector).type(torch.FloatTensor))#.cuda()\n",
    "    predict_y_img = word_img_model(x_img)\n",
    "    print('img: ', rev_ans_dict[str(predict_y_img.data.numpy().argmax())])\n",
    "    '''\n",
    "    #using word vecotr to predict the output\n",
    "    zero_question_img_vector = np.zeros(2048,)\n",
    "    img_word_vector = np.concatenate((question_word_vector, zero_question_img_vector), axis=0)\n",
    "    x_word = Variable(torch.from_numpy(img_word_vector).type(torch.FloatTensor))#.cuda()\n",
    "    predict_y_word = word_img_model(x_word)\n",
    "    print('word: ', rev_ans_dict[str(predict_y_word.data.numpy().argmax())])\n",
    "\n",
    "    display(Image(url= imgid2info[str(img_id)]['coco_url']))\n",
    "for i in range(100):\n",
    "    #show_predict(i, 'how many people are there')\n",
    "    show_predict(i)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
